{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feelings about current business conditions\n",
      "Assessment of current business conditions\n",
      "improved substantially from\n",
      "improved substantially,\n",
      "the first quarter\n",
      "the first quarter\n",
      "40 to 55\n",
      "55 from 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-43e5d4d2ad46>:54: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  if item1.similarity(item2) > threshold:\n"
     ]
    }
   ],
   "source": [
    "# Given two sentences that are known to be paraphrases,\n",
    "# pick the phrases that are similar and contribute to their overall similarity\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans                                                                                                                                                                                       \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# create common grammatical patterns in English. This is incomplete and\n",
    "# non-comprehensive, but sufficiently demonstrates for this example\n",
    "verb_pattern = [{\"POS\": \"VERB\"},\n",
    "               {\"POS\": \"ADV\", \"OP\": \"*\"}, \n",
    "               {\"OP\": \"?\"},\n",
    "               {\"POS\": \"VERB\",\"OP\": \"?\"}]\n",
    "noun_pattern = [{\"POS\": \"NOUN\", \"OP\": \"*\"},\n",
    "                {\"POS\": \"VERB\", \"OP\":\"!\"},\n",
    "                {\"POS\": \"ADJ\", \"OP\": \"?\"},\n",
    "                {\"POS\": \"NOUN\", \"OP\": \"+\"}]\n",
    "proper_noun_pattern = [{\"IS_TITLE\": True, \"OP\": \"+\"}]\n",
    "number_pattern = [{\"IS_DIGIT\": True, \"OP\": \"+\"},\n",
    "                  {\"OP\":\"*\"},\n",
    "                  {\"IS_DIGIT\": True, \"OP\": \"+\"}]\n",
    "\n",
    "# create a matcher with the patterns above\n",
    "matcher.add(\"PHRASE1\", None, verb_pattern)\n",
    "matcher.add(\"PHRASE2\", None, noun_pattern)\n",
    "matcher.add(\"PHRASE3\", None, proper_noun_pattern)\n",
    "matcher.add(\"PHRASE4\", None, number_pattern)\n",
    "\n",
    "# initialize docs with text that are paraphrases of each other\n",
    "doc1 = nlp(\"Feelings about current business conditions improved substantially from the first quarter, jumping from 40 to 55.\")\n",
    "doc2 = nlp(\"Assessment of current business conditions improved substantially, the Conference Board said, jumping to 55 from 40 in the first quarter.\")\n",
    "\n",
    "# find all pattern matches in the two texts independently\n",
    "matches1 = matcher(doc1)\n",
    "matches2 = matcher(doc2)\n",
    "\n",
    "# create a span for each match, and a tuple for all the span in each text\n",
    "spans1 = [Span(doc1, start1, end1, label=\"Phrase1\") for _, start1, end1 in matches1]\n",
    "spans2 = [Span(doc2, start2, end2, label=\"Phrase2\") for _, start2, end2 in matches2]\n",
    "\n",
    "# to avoid duplicates, filter out duplicate or overlapping spans from each tuple\n",
    "spans1 = filter_spans(spans1)\n",
    "spans2 = filter_spans(spans2)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "# compare each match in the first text to second text; if they are similar enough, print the phrases\n",
    "for item1 in spans1:\n",
    "    for item2 in spans2:\n",
    "        if item1.similarity(item2) > threshold:\n",
    "            print(item1.text)\n",
    "            print(item2.text)\n",
    "\n",
    "# could use built in noun_chunks function\n",
    "# for chunk1 in doc1.noun_chunks:\n",
    "#     for chunk2 in doc2.noun_chunks:\n",
    "#         if chunk1.similarity(chunk2) > threshold:\n",
    "#             print(chunk1)\n",
    "#             print(chunk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
